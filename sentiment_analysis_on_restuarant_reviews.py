# -*- coding: utf-8 -*-
"""sentiment_analysis_on_restuarant_reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orBqcqeS2b9XDW5GTr4O0xnpwR8pY9q7
"""

import numpy as np
import pandas as pd

data = pd.read_csv(r"C:\Users\rohan\OneDrive\Desktop\Sentiment Analysis Project\Restaurant_Reviews.tsv", delimiter='\t', quoting=3)

data.shape

data.columns

data.head()

data.info

"""# **Data Preprocessing**"""

#required libraries

import nltk # natural language toolkit
import re # regular expression
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download('stopwords')

corpus = []

for i in range(0, 1000):
    # Remove non-alphabetic characters and convert to lowercase
    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i]).lower()

    # Split the review into words
    review_words = review.split()

    # Remove stopwords
    review_words = [word for word in review_words if word not in set(stopwords.words('english'))]

    # Stemming using Porter Stemmer
    ps = PorterStemmer()
    review_words = [ps.stem(word) for word in review_words]

    # Join the processed words back into a single string
    review = ' '.join(review_words)

    corpus.append(review)

corpus[:1500]

from sklearn.feature_extraction.text import CountVectorizer

# Create a CountVectorizer object with a specified maximum number of features
cv = CountVectorizer(max_features=1500)

# Transform the 'corpus' text data into a sparse matrix
X = cv.fit_transform(corpus).toarray()

# Extract the target variable 'y'
y = data.iloc[:, 1].values

"""# **Split Data**"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""# **Model training**"""

from sklearn.naive_bayes import MultinomialNB

classifier = MultinomialNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

y_pred

from sklearn.metrics import accuracy_score

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

score1 = accuracy_score(y_test,y_pred)
score2 = precision_score(y_test,y_pred)
score3 = recall_score(y_test,y_pred)

print("----Scores----")
print("Accuracy score is: {}%".format(round(score1*100,2)))
print("Precision score is: {}%".format(round(score2*100,2)))
print("Recall score is: {}%".format(round(score3*100,2)))

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

cm

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

plt.figure(figsize = (10,6))
sns.heatmap(cm, annot=True, cmap="YlGnBu", xticklabels=['Negative','Positive'])
plt.xlabel('Predicted values')
plt.ylabel('Actual values')

import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

best_accuracy = 0.0
alpha_val = 0.0

# Define a range of alpha values to tune
alpha_range = np.arange(0.1, 1.1, 0.1)

for alpha in alpha_range:
    temp_classifier = MultinomialNB(alpha=alpha)
    temp_classifier.fit(X_train, y_train)
    temp_y_pred = temp_classifier.predict(X_test)
    score = accuracy_score(y_test, temp_y_pred)
    print("Accuracy score for alpha {} is: {}%".format(round(alpha, 1), round(score * 100, 2)))
    if score > best_accuracy:
        best_accuracy = score
        alpha_val = alpha
    print('------------------------------------------')
    print('The best accuracy is {}% with alpha value as {}'.format(round(best_accuracy * 100, 2), round(alpha_val, 1)))

classifier = MultinomialNB(alpha=0.2)
classifier.fit(X_train, y_train)

"""# **Predictions**"""

def predict_sentiment(sample_review):
  sample_review = re.sub(pattern='[^a-zA-Z]',repl=' ',string = sample_review)
  sample_review_words = sample_review.split()
  sample_review_words = [[word for word in sample_review_words if not word in set(stopwords.words('english'))]]
  ps = PorterStemmer()
  final_review = [ps.stem(word) for word in sample_review_words]
  final_review =' '.join(final_review)

  temp = cv.transform([final_review]).toarray()
  return classifier.predict(temp)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Preprocess function
def preprocess_text(text):
    text = re.sub('[^a-zA-Z]', ' ', text).lower()
    words = text.split()
    words = [word for word in words if word not in set(stopwords.words('english'))]
    ps = PorterStemmer()
    words = [ps.stem(word) for word in words]
    return ' '.join(words)

# Sentiment prediction function
def predict_sentiment(sample_review, classifier, cv):
    sample_review = preprocess_text(sample_review)
    sample_review_vectorized = cv.transform([sample_review]).toarray()
    predicted_sentiment = classifier.predict(sample_review_vectorized)
    return 'POSITIVE' if predicted_sentiment[0] == 1 else 'NEGATIVE'

# Function to predict sentiment for a list of sample reviews
def predict_sentiments(sample_reviews):
    # Assuming you have already trained the Multinomial Naive Bayes classifier on your data
    # Replace 'X_train' and 'y_train' with your actual training data
    classifier = MultinomialNB()
    classifier.fit(X_train, y_train)  # Fit the classifier on your training data

    # Create and fit the CountVectorizer with the same parameters used for training
    cv = CountVectorizer(max_features=1500)
    cv.fit(corpus)  # Fit the CountVectorizer on your preprocessed training data

    # Predict sentiment for each sample review
    sentiments = []
    for sample_review in sample_reviews:
        sentiment = predict_sentiment(sample_review, classifier, cv)
        sentiments.append(sentiment)

    return sentiments

# Sample reviews (you can add more here)
sample_reviews = [
    'The food is really bad.',
    'The service was excellent.',
    'I love this place!',
    'Terrible experience.',
    'The ambiance was fantastic.',
    'The staff was rude.'
]

# Predict sentiments for the sample reviews
predicted_sentiments = predict_sentiments(sample_reviews)

# Print the predicted sentiments for each review
for i, sentiment in enumerate(predicted_sentiments):
    print(f'Review {i + 1}: This is a {sentiment} review: "{sample_reviews[i]}"')

import pickle

# Save the MultinomialNB classifier to a pickle file
with open("classifier_model.pkl", "wb") as file:
    pickle.dump(classifier, file)

"""from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Preprocess function
def preprocess_text(text):
    text = re.sub('[^a-zA-Z]', ' ', text).lower()
    words = text.split()
    words = [word for word in words if word not in set(stopwords.words('english'))]
    ps = PorterStemmer()
    words = [ps.stem(word) for word in words]
    return ' '.join(words)

# Sentiment prediction function
def predict_sentiment(sample_review, classifier, cv):
    sample_review = preprocess_text(sample_review)
    sample_review_vectorized = cv.transform([sample_review]).toarray()
    predicted_sentiment = classifier.predict(sample_review_vectorized)
    return 'POSITIVE' if predicted_sentiment[0] == 1 else 'NEGATIVE'

# Initialize classifier and CountVectorizer
classifier = MultinomialNB()
classifier.fit(X_train, y_train)  # Fit the classifier on your training data

cv = CountVectorizer(max_features=1500)
cv.fit(corpus)  # Fit the CountVectorizer on your preprocessed training data

# Continuously take input for new sample reviews
while True:
    new_review = input("Enter a new review (or 'exit' to quit): ")

    if new_review.lower() == 'exit':
        break

    sentiment = predict_sentiment(new_review, classifier, cv)
    print(f'This is a {sentiment} review: "{new_review}"')"""

